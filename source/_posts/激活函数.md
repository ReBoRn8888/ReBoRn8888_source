---
title: 激活函数
date: 2019-12-26 14:37:58
tags: [神经网络, 深度学习]
categories: 学习笔记
top_img:
cover: https://i.loli.net/2019/12/26/Xn2SwZV3rWtAzCO.png
---

{% aplayer '富士山下' '陈奕迅' 'http://music.163.com/song/media/outer/url?id=65766.mp3' 'http://p2.music.126.net/PcJq6i7zMcPgudejdn1yeg==/109951163256302356.jpg' autoplay %}

# 激活函数的作用
给神经元引入**非线性因素**，使得网络可以逼近任意的**非线性函数**，具有更强的**泛化能力**

# 不同的激活函数
## Sigmoid
$$S(x)=\frac{1}{1+e^{-x}}$$

**函数图像**：

![](https://i.loli.net/2019/12/26/Us17vKjc69phDz2.png)

**缺点**：

- 计算量大（指数运算）
- 反向传播时容易出现梯度消失问题（导数从0开始很快又趋近于0）
- 输出在0-1之间，训练缓慢



## Tanh

$$f(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$

$$tanh(z)=2sigmoid(2z)-1$$

**函数图像**：

![](https://i.loli.net/2019/12/26/8J21gpoXlZG7jrU.png)

**优点**：

- 原点对称（0均值）
- 训练效率比sigmoid快

**缺点**：

- 还是存在梯度消失问题
- 依然是指数运算，计算量大

## ReLU

$$ f(x)=\left\{
\begin{array}{rcl}
x       &      & {if x \geq 0}\\
0     &      & {if x < 0}
\end{array} \right. $$

**函数图像**：

![](https://i.loli.net/2019/12/26/XwJPCxejZ3KNYrG.png)

**优点**：

- 解决了部分梯度消失问题（输入为正时）
- 计算更快（函数简单）
- 收敛更快

**缺点**：

- 会造成神经元死亡且不会复活（输入为负时）

## PReLU、Leaky ReLU、RRelu

**PReLU**: Parametric Rectified Linear Unit

**RReLU**: Randomized Leaky Rectified Linear Unit

$$ f(x_i)=\left\{
\begin{array}{rcl}
x_i       &      & {if x_i \geq 0}\\
a_ix_i     &      & {if x_i < 0}
\end{array} \right. $$

![](https://i.loli.net/2019/12/26/wUH2rsup91FNmV8.png)

**优点**：

- 包含ReLU的所有优点
- 解决了神经元死亡问题

**三种ReLU的比较**：

- 若$a_i=0$，则PReLU退化为ReLU
- Leaky ReLU中的$a_i$是一个很小的固定值
- PReLU中的$a_i$是根据数据变化的
- RReLU中的$a_i$在训练阶段是一个在给定范围内随机抽取的值，在测试阶段会固定下来

## ELU

**ELU**: Exponential Linear Units

$$ f(x)=\left\{
\begin{array}{rcl}
x       &      & {if x \geq 0}\\
\alpha(e^x-1)     &      & {if x < 0}
\end{array} \right. $$

**函数图像**：

![](https://i.loli.net/2019/12/26/1ItyuUKS4FTno72.png)

**优点**：

- 包含ReLU的所有优点
- 神经元不会死亡
- 输出均值接近于0

**缺点**：

- 计算量大（指数运算）

## Maxout
$$\begin{aligned}
f(x)&=max_{j\in[1,k]}z_{ij}\\
&=max(w_1^Tx+b_1,w_2^Tx+b_2, ..., w_k^Tx+b_k)
\end{aligned}$$
即对于某一层神经网络，将激活值最大的作为输出。
下面举例说明，如下图所示：
![](https://i.loli.net/2019/12/26/UEPysmaH36Nuft4.jpg)

假设第**i**层有3个神经元（下），第**i+1**层有4个神经元（上），Maxout相当于在第**i**层和第**i+1**层之间新加了一层，现令**k=3**（中间三块），计算出三块各自的激活值（**$w_i^Tx+b_i$**），然后取三块中的最大值（**$max(w_1^Tx+b_1,w_2^Tx+b_2, ..., w_k^Tx+b_k)$**）作为第**i**层的输出。

**优点**：

- 具有ReLU的所有优点：线性、不饱和性
- 神经元不会死亡

**缺点**：

- 整体参数数量增加（与k有关）

