<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>NLP 学习笔记 II —— BERT | Reborn</title><meta name="description" content="NLP 学习笔记 II —— BERT"><meta name="keywords" content="NLP"><meta name="author" content="Reborn"><meta name="copyright" content="Reborn"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="baidu-site-verification" content="Vkh2aRZcUl"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="NLP 学习笔记 II —— BERT"><meta name="twitter:description" content="NLP 学习笔记 II —— BERT"><meta name="twitter:image" content="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FBERT-family.png"><meta property="og:type" content="article"><meta property="og:title" content="NLP 学习笔记 II —— BERT"><meta property="og:url" content="https://reborn8888.github.io/2020/05/29/NLP-2-BERT/"><meta property="og:site_name" content="Reborn"><meta property="og:description" content="NLP 学习笔记 II —— BERT"><meta property="og:image" content="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FBERT-family.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = '2'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"><link rel="canonical" href="https://reborn8888.github.io/2020/05/29/NLP-2-BERT/"><link rel="prev" title="Docker &amp; Kubernetes" href="https://reborn8888.github.io/2020/06/04/Docker/"><link rel="next" title="NLP 学习笔记 I —— Word2Vec" href="https://reborn8888.github.io/2020/05/27/NLP-1-word2vec/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 5.4.0"><link rel="stylesheet" href="\assets\css\APlayer.min.css" class="aplayer-style-marker">
<script src="\assets\js\APlayer.min.js" class="aplayer-script-marker"></script>
<script src="\assets\js\Meting.min.js" class="meting-script-marker"></script>
</head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Reborn</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://rebornas.blob.core.windows.net/rebornhome/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">33</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">34</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#bert-%E6%98%AF%E4%B8%AA%E5%95%A5"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> BERT 是个啥</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#bert-%E5%BC%BA%E5%9C%A8%E5%93%AA"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> BERT 强在哪</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#bert-%E7%9A%84%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text"> BERT 的网络架构</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#bert-%E7%9A%84%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text"> BERT 的训练策略</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#masked-lm"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text"> Masked LM</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#next-sentence-prediction-nsp"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text"> Next Sentence Prediction (NSP)</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#bert-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text"> BERT 如何使用</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#document-classification"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text"> Document Classification</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#slot-filling-ner"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text"> Slot filling、NER</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#natural-language-inference"><span class="toc_mobile_items-number">5.3.</span> <span class="toc_mobile_items-text"> Natural Language Inference</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#extraction-based-question-answering"><span class="toc_mobile_items-number">5.4.</span> <span class="toc_mobile_items-text"> Extraction-based Question Answering</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#bert-%E5%AE%B6%E6%97%8F"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text"> BERT 家族</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#elmo"><span class="toc_mobile_items-number">6.1.</span> <span class="toc_mobile_items-text"> ELMo</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#ernie"><span class="toc_mobile_items-number">6.2.</span> <span class="toc_mobile_items-text"> ERNIE</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#grover"><span class="toc_mobile_items-number">6.3.</span> <span class="toc_mobile_items-text"> Grover</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#gpt"><span class="toc_mobile_items-number">6.4.</span> <span class="toc_mobile_items-text"> GPT</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#%E5%AE%9E%E8%B7%B5"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text"> 实践</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#reference"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text"> Reference</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#bert-%E6%98%AF%E4%B8%AA%E5%95%A5"><span class="toc-number">1.</span> <span class="toc-text"> BERT 是个啥</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bert-%E5%BC%BA%E5%9C%A8%E5%93%AA"><span class="toc-number">2.</span> <span class="toc-text"> BERT 强在哪</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bert-%E7%9A%84%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">3.</span> <span class="toc-text"> BERT 的网络架构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bert-%E7%9A%84%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="toc-number">4.</span> <span class="toc-text"> BERT 的训练策略</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#masked-lm"><span class="toc-number">4.1.</span> <span class="toc-text"> Masked LM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#next-sentence-prediction-nsp"><span class="toc-number">4.2.</span> <span class="toc-text"> Next Sentence Prediction (NSP)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bert-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text"> BERT 如何使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#document-classification"><span class="toc-number">5.1.</span> <span class="toc-text"> Document Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#slot-filling-ner"><span class="toc-number">5.2.</span> <span class="toc-text"> Slot filling、NER</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#natural-language-inference"><span class="toc-number">5.3.</span> <span class="toc-text"> Natural Language Inference</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#extraction-based-question-answering"><span class="toc-number">5.4.</span> <span class="toc-text"> Extraction-based Question Answering</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bert-%E5%AE%B6%E6%97%8F"><span class="toc-number">6.</span> <span class="toc-text"> BERT 家族</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#elmo"><span class="toc-number">6.1.</span> <span class="toc-text"> ELMo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ernie"><span class="toc-number">6.2.</span> <span class="toc-text"> ERNIE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#grover"><span class="toc-number">6.3.</span> <span class="toc-text"> Grover</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gpt"><span class="toc-number">6.4.</span> <span class="toc-text"> GPT</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5"><span class="toc-number">7.</span> <span class="toc-text"> 实践</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#reference"><span class="toc-number">8.</span> <span class="toc-text"> Reference</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://rebornas.blob.core.windows.net/rebornhome/BERT%2FBERT-family.png)"><div id="post-info"><div id="post-title"><div class="posttitle">NLP 学习笔记 II —— BERT</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-05-29<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-09-25</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">2.3k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 8 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><html><head>
    </head><body><div id="aplayer-RUXsPLWq" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="151646" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="true" data-mutex="false" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"></div>
<p><img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FBERT-family.png" class="lazyload"></p>
<h1 id="bert-是个啥"><a class="markdownIt-Anchor" href="#bert-是个啥"></a> BERT 是个啥</h1>
<p>在 NLP 领域的一大挑战是训练数据的稀缺，因为 NLP 是一个非常大的多样化的领域，需要去处理许多独特的任务（如问答系统、情感分析等），而对于每个任务来说，其相关数据集只有几千或者几十万条已标注数据，而这对于基于深度学习的 NLP 模型来说是远远不够的。</p>
<p>因此，为了跨越这条鸿沟，<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT(Bidirectional Encoder Representations from Transformers)</a> 便应运而生，它的一个高性能的、基于 Transformer Encoder 的 NLP <strong>预训练模型</strong>，BERT 预先训练处一个对自然语言有一定“理解”的通用模型，然后任何人都能够针对不同的下游任务（如问答系统、情感分析等），用 BERT 在自己的数据集上进行<strong>微调（Finetune）</strong>，从而提取到高质量的语言特征（即 Word & Sentence vectors），实现 <strong>SOTA</strong> 的预测效果。</p>
<h1 id="bert-强在哪"><a class="markdownIt-Anchor" href="#bert-强在哪"></a> BERT 强在哪</h1>
<ul>
<li><strong>Bidirectional Transformer</strong></li>
</ul>
<p>与 Word2Vec 等相关的算法相比，BERT 能够根据单词所处的上下文去<strong>动态地</strong>提取其语义信息，而 Word2Vec 及其相关算法对同一个单词具有<strong>固定的</strong>语义信息表示，这在一词多义的情况下就不合适了。</p>
<blockquote>
<p>比如如下两个句子：<br>
1、The man was accused of robbing a <code>bank</code>.<br>
2、The man went fishing by the <code>bank</code> of the river.</p>
<p>前者与“robbing”相关，表示“银行”<br>
后者与“river”相关，表示“河岸”</p>
<p>BERT 有能力去分辨二者，而 Word2Vec 不行</p>
</blockquote>
<p>下图<a href="#reference"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow></mrow><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">^{[2]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">2</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span></a>展示了 BERT 对不同语境中 “苹果” 这个词所提取到的 Embedding 两两之间的相似度矩阵，可见 BERT 在这方面的能力：<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FBERT-embedding-cosine.png" class="lazyload"></p>
<h1 id="bert-的网络架构"><a class="markdownIt-Anchor" href="#bert-的网络架构"></a> BERT 的网络架构</h1>
<p>BERT 的网络架构实际上就是 <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Transformer</a> 中的 Encoder 部分，即下图中的左半部分。<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FTransformer.png" class="lazyload"></p>
<p>关于 Transformer 的知识点，可以参考<a href="https://www.youtube.com/watch?v=ugWDIIOHtPA&pbjreload=101" target="_blank" rel="noopener">视频</a>或<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">Blog</a>来学习，这里简要介绍一下。Transformer 是一种注意力机制，可以学习文本中单词间的上下文关系。Transformer 的关键点在于一个叫做 Self-Attention Layer 的层，输入一个 Sequence，输出也是一个 Sequence，即Seq2Seq。</p>
<p>下面给出用 Transformer 做<strong>机器翻译</strong>的动图（来自<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">Google AI Blog</a>），帮助理解：</p>
<p><img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2Ftransform20fps.gif" class="lazyload"><br>
Transformer 包含两个阶段：</p>
<ul>
<li>Encoder（for Embedding）:
<ul>
<li>输入一个 Sequence：<code>I arrived at the ...</code></li>
<li>通过 Word2Vec 或其他 Word embedding 算法得到所有 Word vectors</li>
<li>每个 Word vector 之间互相做 Attention</li>
<li>经过 n 层 Attention</li>
<li>输出一个 Embedding sequence</li>
</ul>
</li>
<li>Decoder（for Prediction）：
<ul>
<li>为 Decoder 输入一个表示开始的 Token：<code>start</code></li>
<li>Encoder 的输出 Sequence 的每个结点分别与该 Token 做 Attention</li>
<li>经过 n 层 Attention，预测出下一个词：<code>Je</code></li>
<li><code>Je</code> 与 Decoder 在此之前产生的所有东西（<code>start</code>）以及 Encoder 的输出做 Attention</li>
<li>经过 n 层 Attention，预测出下一个词：<code>suis</code></li>
<li>同理，<code>suis</code> 与 Decoder 在此之前产生的所有东西（<code>start</code>，<code>Je</code>）以及 Encoder 的输出做 Attention</li>
<li>直到翻译完成</li>
</ul>
</li>
</ul>
<h1 id="bert-的训练策略"><a class="markdownIt-Anchor" href="#bert-的训练策略"></a> BERT 的训练策略</h1>
<h2 id="masked-lm"><a class="markdownIt-Anchor" href="#masked-lm"></a> Masked LM</h2>
<p>在训练一个语言模型时，我们常常将“预测下一个词”作为预测目标，但这点在 Bidirectional 的模型中是有限制的，比如：“The man went fishing by the bank of ______.”，为了克服这个问题，BERT 使用了一种叫做 <strong>Masked LM</strong> 的策略。</p>
<p>在将 Sequence 输入 BERT 之前，该 Sequence 中会有 15% 的词被替换为 <code>[MASK]</code> 这个 Token，然后模型尝试通过其他未被遮挡的单词来预测这个单词，这样的话，就需要在 Encoder 的输出之后加入一个简单的分类器，用来预测 <code>[MASK]</code> 位置上词汇表中每个单词出现的概率。如下图<a href="#reference"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow></mrow><mrow><mo stretchy="false">[</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">^{[3]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">3</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span></a>所示：<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FMaskedLM.png" class="lazyload"></p>
<h2 id="next-sentence-prediction-nsp"><a class="markdownIt-Anchor" href="#next-sentence-prediction-nsp"></a> Next Sentence Prediction (NSP)</h2>
<p>在 BERT 训练过程中使用的第二个策略叫做 <strong>Next Sentence Prediction</strong>，即输入两句话，预测后一句话在语义上与前一句话是否相接。BERT 中使用 <code>[SEP]</code> 这个 Token 来表示两句话的边界，使用 <code>[CLS]</code> 这个 Token 放在第一句话的最前面表示要进行“前后文预测”，并在此位置输出 NSP 的预测结果（简单的二分类：Yes|No）。如下图<a href="#reference"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow></mrow><mrow><mo stretchy="false">[</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">^{[3]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">3</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span></a>所示：<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FNSP1.png" class="lazyload"><br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FNSP2.png" class="lazyload"></p>
<p>既然有两种训练策略，那么啥时候该用哪种策略呢？<br>
BERT 就很直接：<strong>我全都要</strong>！ 同时使用 <strong>Masked LM</strong> 和 <strong>NSP</strong>：<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FBERT-training.png" class="lazyload"></p>
<h1 id="bert-如何使用"><a class="markdownIt-Anchor" href="#bert-如何使用"></a> BERT 如何使用</h1>
<h2 id="document-classification"><a class="markdownIt-Anchor" href="#document-classification"></a> Document Classification</h2>
<p>文档分类（<strong>Document Classification</strong>）是指对输入的一个句子，或一篇文档做分类。其中又包括情感分析（<strong>Sentiment Analysis</strong>）这一任务。</p>
<blockquote>
<p><strong>Input</strong>：一个句子、表示分类的 Token：<code>[CLS]</code><br>
<strong>Output</strong>：该句子的类别预测结果</p>
</blockquote>
<p>其中分类器从头开始训练，BERT 可以微调。<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2Fcase1.png" class="lazyload"></p>
<h2 id="slot-filling-ner"><a class="markdownIt-Anchor" href="#slot-filling-ner"></a> Slot filling、NER</h2>
<p><strong>Slot filling</strong> 算是广义的 命名体识别(<strong>Named Entity Recognition</strong>,NER)，即给句子中每个单词的类别进行分类，比如“时间”、“地点”等。</p>
<blockquote>
<p><strong>Input</strong>：一个句子、表示分类的 Token：<code>[CLS]</code><br>
<strong>Output</strong>：每个词的类别</p>
</blockquote>
<p><img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2Fcase2.png" class="lazyload"></p>
<h2 id="natural-language-inference"><a class="markdownIt-Anchor" href="#natural-language-inference"></a> Natural Language Inference</h2>
<p>自然语言推断（<strong>Natural Language Inference</strong>）是指给定一个“前提”（Premise）和一个“假设”（Hypothesis），让模型去预测在这个“前提”下的“假设”是否成立。</p>
<blockquote>
<p><strong>Input</strong>：两个句子、用于分割两句的 Token：<code>[SEP]</code>、表示分类的 Token：<code>[CLS]</code><br>
<strong>Output</strong>：“假设”是否成立（T/F/unknown）</p>
</blockquote>
<p><img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2Fcase3.png" class="lazyload"></p>
<h2 id="extraction-based-question-answering"><a class="markdownIt-Anchor" href="#extraction-based-question-answering"></a> Extraction-based Question Answering</h2>
<p>问答系统（<strong>Question Answering</strong>，QA）是指输入一段“文本”，并输入一个“问题”，让模型去预测出这个“问题”的“答案”。其中 <strong>Extraction-based</strong> 是指这个“答案”可以从给定的“文本”中提取出来。</p>
<blockquote>
<p><strong>Input</strong>：一个问题 Q、一段文本 D、用于分割两句的 Token：<code>[SEP]</code>、表示分类的 Token：<code>[CLS]</code><br>
<strong>Output</strong>：两个整数 s 和 e（分别表示答案处于文本 D 中的起始 start 和结束 end 位置）</p>
</blockquote>
<p>通过训练集学习出下图中<strong>橙色</strong>和<strong>蓝色</strong>矩形所示的 Embedding 表示，将它们分别与文本 D 中所提取出的 Word embedding 计算<strong>内积</strong>，然后通过一个 Softmax 运算，找出概率最高的那个词所处的位置，就分别表示答案位于文本 D 中的具体位置，其中<strong>橙色</strong>对应答案开始（Start，<strong>s</strong>）的位置，<strong>蓝色</strong>对应答案结束（End，<strong>e</strong>）的位置。<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2Fcase4-1.png" class="lazyload"><br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2Fcase4-2.png" class="lazyload"></p>
<h1 id="bert-家族"><a class="markdownIt-Anchor" href="#bert-家族"></a> BERT 家族</h1>
<p>还记得本文一开头的那张图片嘛？那是美国儿童电视节目《芝麻街》中主要角色的一张合照，可以看到，本文所介绍的 BERT 就是《芝麻街》中的黄色小人的名字，同样的，红色小人叫做 ELMO，橙色小人叫做 ERNIE，蓝色小人叫做 Grover，他们分别对应 NLP 中的三个语言模型：<strong>ELMo</strong>（<strong>E</strong>mbeddings from <strong>L</strong>anguage <strong>Mo</strong>dels）、<strong>ERNIE</strong>（<strong>E</strong>nhanced <strong>R</strong>epresentation through K<strong>n</strong>owledge <strong>I</strong>nt<strong>e</strong>gration）以及 <strong>Grover</strong>（<strong>G</strong>enerating a<strong>R</strong>ticles by <strong>O</strong>nly <strong>V</strong>iewing m<strong>E</strong>tadata <strong>R</strong>ecords）。</p>
<blockquote>
<p>奇了，不知道为什么 NLP 研究者们这么喜欢《芝麻街》.<br>
<img alt data-src="https://i.loli.net/2020/05/29/jDPdz3HNUi1ect9.jpg" class="lazyload"></p>
</blockquote>
<h2 id="elmo"><a class="markdownIt-Anchor" href="#elmo"></a> ELMo</h2>
<p><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">ELMo（<strong>E</strong>mbeddings from <strong>L</strong>anguage <strong>Mo</strong>dels）</a>是一种基于双向 RNN 的语言模型，如下图所示：<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FELMo1.png" class="lazyload"><br>
上图展示的是两层 ELMo 的示意图，可以看到，每一层对于每个输入，都会输出两个 Embedding 向量（一个正向 Embedding，一个反向 Embedding），ELMo 直接将它们 Concat 起来。</p>
<p>那么如果要堆叠多层 RNN 的话该怎么办呢，对于一个输入 Word，每一层都会输出一个 Embedding，n 层 RNN 就会生成 n 个 Embedding，不就越来越多了嘛？别急，ELMo 说：<strong>我全都要</strong>！<br>
<img alt data-src="https://i.loli.net/2020/05/29/9VY4QW7vZM6NoLy.png" class="lazyload"></p>
<p>对于一个两层 ELMo 来说，它会将这两层的两个 Embedding 通过某种操作二合一：<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FELMo2.png" class="lazyload"><br>
上图中蓝色 Embedding 即为合成的结果，可以看到，实际上就是一个加权平均，其中值得关注的是这两个权重 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\alpha_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\alpha_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是如何得来的。ELMo 中定义，这两个权重根据不同的下游任务而有所不同，如图中橙色框内所示，“<strong>Token</strong>”、“<strong>LSTM1</strong>”和“<strong>LSTM2</strong>”分别表示“<strong>输入数据的 Word vector</strong>”、“<strong>第一层 LSTM 的输出</strong>”以及“<strong>第二层 LSTM 的输出</strong>”，我们举个例子，比如对于“<strong>Coref</strong>”以及“<strong>SQuAD</strong>”这两个任务，他们对“<strong>第一层 LSTM 的输出</strong>”更看重，而“<strong>第二层 LSTM 的输出</strong>”对它们几乎无用，其他任务以此类推。</p>
<h2 id="ernie"><a class="markdownIt-Anchor" href="#ernie"></a> ERNIE</h2>
<p><a href="https://arxiv.org/pdf/1905.07129.pdf" target="_blank" rel="noopener"><strong>ERNIE</strong>（<strong>E</strong>nhanced <strong>R</strong>epresentation through K<strong>n</strong>owledge <strong>I</strong>nt<strong>e</strong>gration）</a>是由百度飞桨 PaddlePaddle 团队提出的，一种专门针对中文的语义模型。</p>
<p>用一张图<a href="#reference"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mrow></mrow><mrow><mo stretchy="false">[</mo><mn>6</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">^{[6]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mtight">6</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span></span></span></span></a>来说明 ERNIE 和 BERT 的区别：<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FERNIE.png" class="lazyload"><br>
由于 BERT 中的 [MASK] 机制是针对于字来说的，而中文中有非常多的词组，如果只盖掉其中某个字，那么很容易就能猜出来，因此，ERNIE 选择盖掉整个词组，这样才更合理。</p>
<ul>
<li>Github Repo：<a href="https://github.com/PaddlePaddle/ERNIE" target="_blank" rel="noopener">ERNIE</a></li>
</ul>
<h2 id="grover"><a class="markdownIt-Anchor" href="#grover"></a> Grover</h2>
<p>此处不做介绍。</p>
<h2 id="gpt"><a class="markdownIt-Anchor" href="#gpt"></a> GPT</h2>
<p><a href="http://www.nlpir.org/wordpress/wp-content/uploads/2019/06/Improving-language-understanding-by-generative-pre-training.pdf" target="_blank" rel="noopener">GPT（Generative Pre-Training）</a>虽然不是《芝麻街》中的人物，但它经常被拿来与 BERT 进行比较，因此这边也顺带提一下，它和<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">GPT-2</a>都是由 OpenAI 团队提出的一种语义模型，与 BERT 相对，它实际上是 Transformer 的 Decoder 部分。而这个模型的卖点呢，就是 —————— <strong>大大大大大</strong>。详情可参考原论文，这里不做详细介绍。<br>
<img alt data-src="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FGPT.png" class="lazyload"><br>
可以看到，ELMo 模型只有 94MB，BERT 模型有340MB，而 GPT-2 模型达到了 1542MB的容量，这对于一个语义模型来说已经非常巨大了。</p>
<h1 id="实践"><a class="markdownIt-Anchor" href="#实践"></a> 实践</h1>
<ul>
<li>Word Embedding:
<ul>
<li><a href="http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/" target="_blank" rel="noopener">Tutorial</a></li>
<li><a href="https://colab.research.google.com/drive/1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU" target="_blank" rel="noopener">Code</a></li>
</ul>
</li>
<li>Sentence Classification:
<ul>
<li><a href="http://mccormickml.com/2019/07/22/BERT-fine-tuning/" target="_blank" rel="noopener">Tutorial</a></li>
<li><a href="https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP" target="_blank" rel="noopener">Code</a></li>
</ul>
</li>
<li>Document Classification:
<ul>
<li><a href="https://youtu.be/_eSGWNqKeeY" target="_blank" rel="noopener">Video</a></li>
</ul>
</li>
</ul>
<h1 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h1>
<p>[1] Video: <a href="https://www.youtube.com/playlist?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6" target="_blank" rel="noopener">BERT Research Series</a><br>
[2] Video: <a href="https://www.youtube.com/watch?v=1_gRK9EIQpc&t=890s" target="_blank" rel="noopener">BERT and its family - Introduction and Fine-tune</a><br>
[3] Video: <a href="https://www.youtube.com/watch?v=UYPa347-DdE" target="_blank" rel="noopener">ELMO, BERT, GPT</a><br>
[4] Video: <a href="https://www.youtube.com/watch?v=ugWDIIOHtPA&pbjreload=101" target="_blank" rel="noopener">Transformer</a><br>
[5] Blog Post: <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" target="_blank" rel="noopener">Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing</a><br>
[6] 知乎专栏: <a href="https://zhuanlan.zhihu.com/p/59436589" target="_blank" rel="noopener">ERNIE</a></p>
</body></html></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Reborn</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://reborn8888.github.io/2020/05/29/NLP-2-BERT/">https://reborn8888.github.io/2020/05/29/NLP-2-BERT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ReBoRn8888.github.io">Reborn</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP    </a></div><div class="post_share"><div class="social-share" data-image="https://rebornas.blob.core.windows.net/rebornhome/BERT%2FBERT-family.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://i.loli.net/2019/12/08/sLmoV1atujlQPRY.png" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://i.loli.net/2019/12/08/HhJfxq64XDn3LMK.png" alt="支付寶"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/06/04/Docker/"><img class="prev_cover lazyload" data-src="https://rebornas.blob.core.windows.net/rebornhome/Docker%2Fdocker.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>Docker &amp; Kubernetes</span></div></a></div><div class="next-post pull_right"><a href="/2020/05/27/NLP-1-word2vec/"><img class="next_cover lazyload" data-src="https://rebornas.blob.core.windows.net/rebornhome/word2vec%2Fw2v.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>NLP 学习笔记 I —— Word2Vec</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/05/27/NLP-1-word2vec/" title="NLP 学习笔记 I —— Word2Vec"><img class="relatedPosts_cover lazyload"data-src="https://rebornas.blob.core.windows.net/rebornhome/word2vec%2Fw2v.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-09-14</div><div class="relatedPosts_title">NLP 学习笔记 I —— Word2Vec</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'2boKvi6UsSeUABpVH99z87cj-gzGzoHsz',
  appKey:'instDcdLyY9Ace9gcgI1eQh3',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2021 By Reborn</div><div class="framework-info"><span>驱动 </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <u><a href="https://reborn8888.github.io/">Blog</a></u>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">簡</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script id="ribbon_piao" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/piao.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>